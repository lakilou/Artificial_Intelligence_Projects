{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8eee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=4000)\n",
    "\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00619792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb50e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"[bos] this film was just brilliant casting location scenery story direction [oov] really suited the part they played and you could just imagine being there robert [oov] is an amazing actor and now the same being director [oov] father came from the same [oov] island as myself so i loved the fact there was a real connection with this film the witty [oov] throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for [oov] and would recommend it to everyone to watch and the fly [oov] was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also [oov] to the two little [oov] that played the [oov] of norman and paul they were just brilliant children are often left out of the [oov] list i think because the stars that play them all grown up are such a big [oov] for the whole film but these children are amazing and should be [oov] for what they have done don't you think the whole story was so lovely because it was true and was [oov] life after all that was [oov] with us all\",\n",
       "       \"[bos] big hair big [oov] bad music and a giant [oov] [oov] these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an [oov] the script is completely laughable the best is the end [oov] with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are [oov] and funny in equal [oov] the hair is big lots of [oov] [oov] men wear those cut [oov] [oov] that show off their [oov] [oov] that men actually [oov] them and the music is just [oov] trash that plays over and over again in almost every scene there is [oov] music [oov] and [oov] taking away bodies and the [oov] still doesn't close for [oov] all [oov] aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\",\n",
       "       \"[bos] this has to be one of the worst films of the [oov] when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our [oov] touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had [oov] working to watch this [oov] excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how [oov] this is to watch save yourself an hour a bit of your life\"],\n",
       "      dtype='<U12529')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eda3453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3998\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list()\n",
    "for text in x_train:\n",
    "  tokens = text.split()\n",
    "  vocabulary.extend(tokens)\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "print(len(vocabulary)) # number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1e6e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25000/25000 [06:07<00:00, 68.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 25000/25000 [05:56<00:00, 70.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x_train_binary = list()\n",
    "x_test_binary = list()\n",
    "\n",
    "for text in tqdm(x_train):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_train_binary.append(binary_vector)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary) # an array that in every position there is a sentiment\n",
    "\n",
    "for text in tqdm(x_test):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_test_binary.append(binary_vector)\n",
    "\n",
    "x_test_binary = np.array(x_test_binary) # an array that in every position there is a sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50eb2ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22eb61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3998)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_binary.shape) # 25000 samples and 3998 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ad616",
   "metadata": {},
   "source": [
    "### The function for logistic regression is given below:\n",
    "\n",
    "h( x ) = sigmoid( wx + b )\n",
    "\n",
    "Here, w is the weight vector.\n",
    "x is the feature vector. \n",
    "b is the bias.\n",
    "\n",
    "sigmoid( z ) = 1 / ( 1 + e( - z ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb953043",
   "metadata": {},
   "source": [
    "Optimizing algorithms like i.e gradient descent only converge convex function into a global minimum. \n",
    "\n",
    "So, the simplified cost function we use :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e1ad2",
   "metadata": {},
   "source": [
    "J = - ylog( h(x) ) - ( 1 - y )log( 1 - h(x) )\n",
    "\n",
    "here, y is the real target value\n",
    "\n",
    "h( x ) = sigmoid( wx + b )\n",
    "\n",
    "For y = 0,\n",
    "\n",
    "J = - log( 1 - h(x) )\n",
    "\n",
    "and y = 1,\n",
    "\n",
    "J = - log( h(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb38943",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "<code>repeat until convergence  {\n",
    "       tmpi = wi - learning_rate * dwi\n",
    "       wi = tmpi         \n",
    "}</code>"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "0794889f",
   "metadata": {},
   "source": [
    "The chain rule is used to calculate the gradients like i.e dw.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "here, a = sigmoid( z ) and z = wx + b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa49d8",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0e550f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression() :\n",
    "    def __init__( self, learning_rate, iterations ) :\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        \n",
    "    # Function for model training\n",
    "    def fit( self, X, Y ) :\n",
    "        # no_of_training_examples, no_of_features\n",
    "        self.m, self.n = X.shape\n",
    "        # weight initialization\n",
    "        self.W = np.zeros( self.n )\n",
    "        self.b = 0\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        # gradient descent learning       \n",
    "        for i in range( self.iterations ) :\n",
    "            self.update_weights()\n",
    "        return self\n",
    "    \n",
    "    # Helper function to update weights in gradient descent\n",
    "    def update_weights( self ) :\n",
    "        A = 1 / ( 1 + np.exp( - ( self.X.dot( self.W ) + self.b ) ) )\n",
    "        \n",
    "        # calculate gradients\n",
    "        tmp = ( A - self.Y.T )\n",
    "        tmp = np.reshape( tmp, self.m )\n",
    "        dW = np.dot( self.X.T, tmp ) / self.m\n",
    "        db = np.sum( tmp ) / self.m\n",
    "        \n",
    "        # update weights\n",
    "        self.W = self.W - self.learning_rate * dW\n",
    "        self.b = self.b - self.learning_rate * db\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Hypothetical function h(x)\n",
    "    def predict( self, X ) :\n",
    "        Z = 1 / ( 1 + np.exp( - ( X.dot( self.W ) + self.b ) ) )\n",
    "        Y = [1 if i > 0.5 else 0 for i in Z]\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a2204ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR classification accuracy: 0.82576\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "    \n",
    "    regressor = LogisticRegression(learning_rate=0.01, iterations=1000)\n",
    "    regressor.fit(x_train_binary, y_train)\n",
    "    predictions = regressor.predict(x_test_binary)\n",
    "\n",
    "    print(\"Logistic Regression classification accuracy:\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06ba7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare our model's accuracy with sklearn model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d238a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classification accuracy: 0.85812\n"
     ]
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train_binary, y_train)\n",
    "predictions = logisticRegr.predict(x_test_binary)\n",
    "\n",
    "print(\"Logistic Regression classification accuracy:\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e6564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
